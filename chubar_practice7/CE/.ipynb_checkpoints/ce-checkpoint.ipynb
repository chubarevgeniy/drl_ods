{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8867ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7817265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda')\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 64), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, self.action_dim)\n",
    "        )\n",
    "        #self.network.to(self.device)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        #_input = _input.to(self.device)\n",
    "        return self.network(_input)\n",
    "    \n",
    "    def get_action(self, state, exploration=0):\n",
    "        state = torch.FloatTensor(state)\n",
    "        #logits = self.forward(state).to('cpu')\n",
    "        pure_action = self.forward(state).detach().numpy()\n",
    "        #pure_action += exploration*np.random.randn(self.action_dim)\n",
    "        action = pure_action + exploration*np.random.randn(self.action_dim)\n",
    "        return np.clip(action,-1,1)\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        #elite_states = elite_states.to(self.device)\n",
    "        elite_actions = torch.FloatTensor(np.array(elite_actions))\n",
    "        #elite_actions = elite_actions.to(self.device)\n",
    "        for i in range(15):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c375acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, exploration=0, visualize=False, custom_reward_func = None):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0, 'custom_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "    for i in range(trajectory_len):\n",
    "        with torch.no_grad():\n",
    "            action = agent.get_action(state, exploration)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "        if custom_reward_func:\n",
    "            reward = custom_reward_func(state) + reward\n",
    "            trajectory['custom_reward'] = np.max([trajectory['custom_reward'],reward])\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if visualize:\n",
    "            env.render()\n",
    "        \n",
    "        if(i != trajectory_len-1):\n",
    "            trajectory['states'].append(state)\n",
    "    \n",
    "    if visualize:\n",
    "        print(i,trajectory['total_reward'])  \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef5840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param, solved_traj):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param)\n",
    "    new_solved = [trajectory for trajectory in trajectories if trajectory['total_reward'] >= 200]\n",
    "    if(len(new_solved)):\n",
    "        solved_traj.extend(new_solved)\n",
    "        pass\n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] >= quantile]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ae427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous = True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "learnin_hist = {'mean_reward': [], 'exploration': [], 'validation': []}\n",
    "\n",
    "agent = CEM(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851e2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_len = 400\n",
    "q_param = 0.8\n",
    "exploration = 0.9\n",
    "e_rate = 0.993\n",
    "solved_traj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "527f9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = -286.15467356030763\n",
      "solved = 0  elite len = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mget_trajectory\u001b[1;34m(env, agent, trajectory_len, exploration, visualize, custom_reward_func)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_len):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     trajectory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     11\u001b[0m     state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mCEM.get_action\u001b[1;34m(self, state, exploration)\u001b[0m\n\u001b[0;32m     24\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#logits = self.forward(state).to('cpu')\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pure_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#pure_action += exploration*np.random.randn(self.action_dim)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m action \u001b[38;5;241m=\u001b[39m pure_action \u001b[38;5;241m+\u001b[39m exploration\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mCEM.forward\u001b[1;34m(self, _input)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, _input):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#_input = _input.to(self.device)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episode_n = 1000\n",
    "trajectory_n = 20\n",
    "\n",
    "for episode in range(0,episode_n):\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len, exploration) for _ in range(trajectory_n)]\n",
    "    exploration *= e_rate\n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param, solved_traj)\n",
    "    print('solved =',len(solved_traj),' elite len =',len(elite_trajectories))\n",
    "    learnin_hist['exploration'].append([episode,mean_total_reward])\n",
    "    learnin_hist['mean_reward'].append([episode,mean_total_reward])\n",
    "    if episode%10==0:\n",
    "        trajectories = [get_trajectory(env, agent, trajectory_len, -1) for _ in range(trajectory_n*3)]\n",
    "        mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "        if(mean_total_reward>2600):\n",
    "            break\n",
    "        learnin_hist['validation'].append([episode,mean_total_reward])\n",
    "        print('validation =', mean_total_reward)\n",
    "    if len(elite_trajectories) > 0:\n",
    "        if(len(solved_traj)>0):\n",
    "            elite_trajectories.append(solved_traj[np.random.randint(len(solved_traj))])\n",
    "            agent.update_policy(elite_trajectories)\n",
    "        else:\n",
    "            agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29665a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.array(learnin_hist['mean_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ceefc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[:,0] *= trajectory_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07ce4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x172417c2200>,\n",
       " <matplotlib.lines.Line2D at 0x172417c2260>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaed547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee77f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c233e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42392ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dcdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22052ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
