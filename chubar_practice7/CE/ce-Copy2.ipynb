{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8867ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7817265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda')\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 64), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, self.action_dim)\n",
    "        )\n",
    "        #self.network.to(self.device)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        #_input = _input.to(self.device)\n",
    "        return self.network(_input)\n",
    "    \n",
    "    def get_action(self, state, exploration=0):\n",
    "        state = torch.FloatTensor(state)\n",
    "        #logits = self.forward(state).to('cpu')\n",
    "        pure_action = self.forward(state).detach().numpy()\n",
    "        #pure_action += exploration*np.random.randn(self.action_dim)\n",
    "        action = pure_action + exploration*np.random.randn(self.action_dim)\n",
    "        return np.clip(action,-1,1)\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        #elite_states = elite_states.to(self.device)\n",
    "        elite_actions = torch.FloatTensor(np.array(elite_actions))\n",
    "        #elite_actions = elite_actions.to(self.device)\n",
    "        for i in range(15):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c375acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, exploration=0, visualize=False, custom_reward_func = None):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0, 'custom_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "    for i in range(trajectory_len):\n",
    "        with torch.no_grad():\n",
    "            action = agent.get_action(state, exploration)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "        if custom_reward_func:\n",
    "            reward = custom_reward_func(state) + reward\n",
    "            trajectory['custom_reward'] = np.max([trajectory['custom_reward'],reward])\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if visualize:\n",
    "            env.render()\n",
    "        \n",
    "        if(i != trajectory_len-1):\n",
    "            trajectory['states'].append(state)\n",
    "    \n",
    "    if visualize:\n",
    "        print(i,trajectory['total_reward'])  \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef5840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param, solved_traj):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param)\n",
    "    new_solved = [trajectory for trajectory in trajectories if trajectory['total_reward'] >= 200]\n",
    "    if(len(new_solved)):\n",
    "        solved_traj.extend(new_solved)\n",
    "        pass\n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] >= quantile]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ae427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous = True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "learnin_hist = {'mean_reward': [], 'exploration': [], 'validation': []}\n",
    "\n",
    "agent = CEM(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851e2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_len = 400\n",
    "q_param = 0.8\n",
    "exploration = 0.9\n",
    "e_rate = 0.993\n",
    "solved_traj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = -250.93943427441954\n",
      "solved = 0  elite len = 4\n",
      "validation = -282.4651062994835\n",
      "episode: 1, mean_total_reward = -183.60796570273564\n",
      "solved = 0  elite len = 4\n",
      "episode: 2, mean_total_reward = -110.93871048765436\n",
      "solved = 0  elite len = 4\n",
      "episode: 3, mean_total_reward = -78.37587517660259\n",
      "solved = 0  elite len = 4\n",
      "episode: 4, mean_total_reward = -67.92163696680079\n",
      "solved = 0  elite len = 4\n",
      "episode: 5, mean_total_reward = -39.29223477693188\n",
      "solved = 0  elite len = 4\n",
      "episode: 6, mean_total_reward = -45.48210544650448\n",
      "solved = 0  elite len = 4\n",
      "episode: 7, mean_total_reward = -21.33972667760458\n",
      "solved = 0  elite len = 4\n",
      "episode: 8, mean_total_reward = -23.489974166629196\n",
      "solved = 0  elite len = 4\n",
      "episode: 9, mean_total_reward = -21.433290974506576\n",
      "solved = 0  elite len = 4\n",
      "episode: 10, mean_total_reward = 0.1171188254343349\n",
      "solved = 0  elite len = 4\n",
      "validation = 0.5204764631349071\n",
      "episode: 11, mean_total_reward = 7.890742454583216\n",
      "solved = 0  elite len = 4\n",
      "episode: 12, mean_total_reward = 23.658929932979227\n",
      "solved = 0  elite len = 4\n",
      "episode: 13, mean_total_reward = 17.073904216469543\n",
      "solved = 0  elite len = 4\n",
      "episode: 14, mean_total_reward = 3.0877808270768154\n",
      "solved = 0  elite len = 4\n",
      "episode: 15, mean_total_reward = 3.7320377239249063\n",
      "solved = 0  elite len = 4\n",
      "episode: 16, mean_total_reward = -0.5859022885587759\n",
      "solved = 0  elite len = 4\n",
      "episode: 17, mean_total_reward = 17.276902185154803\n",
      "solved = 0  elite len = 4\n",
      "episode: 18, mean_total_reward = 22.73545018391702\n",
      "solved = 0  elite len = 4\n",
      "episode: 19, mean_total_reward = -8.883284423428453\n",
      "solved = 0  elite len = 4\n",
      "episode: 20, mean_total_reward = 26.511121919232902\n",
      "solved = 0  elite len = 4\n",
      "validation = 32.431349860832334\n",
      "episode: 21, mean_total_reward = 52.574933343557156\n",
      "solved = 0  elite len = 4\n",
      "episode: 22, mean_total_reward = 41.929365304613185\n",
      "solved = 0  elite len = 4\n",
      "episode: 23, mean_total_reward = 38.795043200649424\n",
      "solved = 0  elite len = 4\n",
      "episode: 24, mean_total_reward = 23.78367756756601\n",
      "solved = 0  elite len = 4\n",
      "episode: 25, mean_total_reward = 25.054015019646762\n",
      "solved = 0  elite len = 4\n",
      "episode: 26, mean_total_reward = 67.12499339185983\n",
      "solved = 0  elite len = 4\n",
      "episode: 27, mean_total_reward = 39.765562493127966\n",
      "solved = 0  elite len = 4\n",
      "episode: 28, mean_total_reward = 64.05698148496892\n",
      "solved = 0  elite len = 4\n",
      "episode: 29, mean_total_reward = 46.60722150948429\n",
      "solved = 0  elite len = 4\n",
      "episode: 30, mean_total_reward = 52.63614842993866\n",
      "solved = 0  elite len = 4\n",
      "validation = 47.881036065650356\n",
      "episode: 31, mean_total_reward = 37.33406828418829\n",
      "solved = 0  elite len = 4\n",
      "episode: 32, mean_total_reward = 22.39576522502993\n",
      "solved = 0  elite len = 4\n",
      "episode: 33, mean_total_reward = 17.807391565358863\n",
      "solved = 0  elite len = 4\n",
      "episode: 34, mean_total_reward = 45.3121312479844\n",
      "solved = 0  elite len = 4\n",
      "episode: 35, mean_total_reward = 39.12177581478274\n",
      "solved = 0  elite len = 4\n",
      "episode: 36, mean_total_reward = 41.902201036695644\n",
      "solved = 0  elite len = 4\n",
      "episode: 37, mean_total_reward = 21.829729927740694\n",
      "solved = 0  elite len = 4\n",
      "episode: 38, mean_total_reward = 40.12581622804697\n",
      "solved = 0  elite len = 4\n",
      "episode: 39, mean_total_reward = 36.32647115597088\n",
      "solved = 0  elite len = 4\n",
      "episode: 40, mean_total_reward = 52.954072231694134\n",
      "solved = 0  elite len = 4\n",
      "validation = 35.595668386240284\n",
      "episode: 41, mean_total_reward = 61.49512468151605\n",
      "solved = 0  elite len = 4\n",
      "episode: 42, mean_total_reward = 49.4695791574583\n",
      "solved = 0  elite len = 4\n",
      "episode: 43, mean_total_reward = 73.89557101120184\n",
      "solved = 0  elite len = 4\n",
      "episode: 44, mean_total_reward = 65.1762710402049\n",
      "solved = 0  elite len = 4\n",
      "episode: 45, mean_total_reward = 12.21627333213264\n",
      "solved = 0  elite len = 4\n",
      "episode: 46, mean_total_reward = 44.136004871950476\n",
      "solved = 0  elite len = 4\n",
      "episode: 47, mean_total_reward = 30.571356828516162\n",
      "solved = 0  elite len = 4\n",
      "episode: 48, mean_total_reward = 19.331055982717743\n",
      "solved = 0  elite len = 4\n",
      "episode: 49, mean_total_reward = 43.51892995044218\n",
      "solved = 0  elite len = 4\n",
      "episode: 50, mean_total_reward = 59.6177067662871\n",
      "solved = 0  elite len = 4\n",
      "validation = 43.13696425833455\n",
      "episode: 51, mean_total_reward = 31.782971570608424\n",
      "solved = 0  elite len = 4\n",
      "episode: 52, mean_total_reward = 61.98316489118279\n",
      "solved = 0  elite len = 4\n",
      "episode: 53, mean_total_reward = 68.2669250537119\n",
      "solved = 0  elite len = 4\n",
      "episode: 54, mean_total_reward = 68.20660211238831\n",
      "solved = 0  elite len = 4\n",
      "episode: 55, mean_total_reward = 68.1494365067804\n",
      "solved = 0  elite len = 4\n",
      "episode: 56, mean_total_reward = 76.23738184439955\n",
      "solved = 0  elite len = 4\n",
      "episode: 57, mean_total_reward = 101.02455986003561\n",
      "solved = 0  elite len = 4\n",
      "episode: 58, mean_total_reward = 70.42507881857472\n",
      "solved = 0  elite len = 4\n",
      "episode: 59, mean_total_reward = 65.13666085694352\n",
      "solved = 0  elite len = 4\n",
      "episode: 60, mean_total_reward = 79.73410278442881\n",
      "solved = 0  elite len = 4\n",
      "validation = 47.310378431440846\n",
      "episode: 61, mean_total_reward = 60.756932683939894\n",
      "solved = 0  elite len = 4\n",
      "episode: 62, mean_total_reward = 89.90278981430103\n",
      "solved = 0  elite len = 4\n",
      "episode: 63, mean_total_reward = 74.29090728624718\n",
      "solved = 0  elite len = 4\n",
      "episode: 64, mean_total_reward = 74.1693582207132\n",
      "solved = 0  elite len = 4\n",
      "episode: 65, mean_total_reward = 79.89771189185161\n",
      "solved = 0  elite len = 4\n",
      "episode: 66, mean_total_reward = 97.30730464782087\n",
      "solved = 0  elite len = 4\n",
      "episode: 67, mean_total_reward = 85.24026686061168\n",
      "solved = 0  elite len = 4\n",
      "episode: 68, mean_total_reward = 101.70527382232083\n",
      "solved = 0  elite len = 4\n",
      "episode: 69, mean_total_reward = 85.5342855105461\n",
      "solved = 0  elite len = 4\n",
      "episode: 70, mean_total_reward = 93.14594200420127\n",
      "solved = 0  elite len = 4\n",
      "validation = 73.50113782979388\n",
      "episode: 71, mean_total_reward = 97.25323567603917\n",
      "solved = 0  elite len = 4\n",
      "episode: 72, mean_total_reward = 97.98703369197116\n",
      "solved = 0  elite len = 4\n",
      "episode: 73, mean_total_reward = 84.14884813945245\n",
      "solved = 0  elite len = 4\n",
      "episode: 74, mean_total_reward = 70.6801409657494\n",
      "solved = 0  elite len = 4\n",
      "episode: 75, mean_total_reward = 88.59572580184098\n",
      "solved = 0  elite len = 4\n",
      "episode: 76, mean_total_reward = 74.00949682677013\n",
      "solved = 0  elite len = 4\n",
      "episode: 77, mean_total_reward = 64.33795614150429\n",
      "solved = 0  elite len = 4\n",
      "episode: 78, mean_total_reward = 83.7324622207926\n",
      "solved = 0  elite len = 4\n",
      "episode: 79, mean_total_reward = 57.466660278964056\n",
      "solved = 0  elite len = 4\n",
      "episode: 80, mean_total_reward = 99.07449151282802\n",
      "solved = 0  elite len = 4\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episode_n = 1000\n",
    "trajectory_n = 20\n",
    "\n",
    "for episode in range(0,episode_n):\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len, exploration) for _ in range(trajectory_n)]\n",
    "    exploration *= e_rate\n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param, solved_traj)\n",
    "    print('solved =',len(solved_traj),' elite len =',len(elite_trajectories))\n",
    "    learnin_hist['exploration'].append([episode,mean_total_reward])\n",
    "    learnin_hist['mean_reward'].append([episode,mean_total_reward])\n",
    "    if episode%10==0:\n",
    "        trajectories = [get_trajectory(env, agent, trajectory_len, -1) for _ in range(trajectory_n*3)]\n",
    "        mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "        if(mean_total_reward>2600):\n",
    "            break\n",
    "        learnin_hist['validation'].append([episode,mean_total_reward])\n",
    "        print('validation =', mean_total_reward)\n",
    "    if len(elite_trajectories) > 0:\n",
    "        if(len(solved_traj)>0):\n",
    "            elite_trajectories.append(solved_traj[np.random.randint(len(solved_traj))])\n",
    "            agent.update_policy(elite_trajectories)\n",
    "        else:\n",
    "            agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29665a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.array(learnin_hist['mean_reward'])\n",
    "hist = hist[:400,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceefc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[:,0] *= trajectory_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ce4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist[:,0],hist[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaed547",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('ce2.txt',hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee77f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c233e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42392ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dcdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22052ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
