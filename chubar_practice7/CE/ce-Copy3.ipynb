{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8867ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7817265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda')\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 64), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, self.action_dim)\n",
    "        )\n",
    "        #self.network.to(self.device)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        #_input = _input.to(self.device)\n",
    "        return self.network(_input)\n",
    "    \n",
    "    def get_action(self, state, exploration=0):\n",
    "        state = torch.FloatTensor(state)\n",
    "        #logits = self.forward(state).to('cpu')\n",
    "        pure_action = self.forward(state).detach().numpy()\n",
    "        #pure_action += exploration*np.random.randn(self.action_dim)\n",
    "        action = pure_action + exploration*np.random.randn(self.action_dim)\n",
    "        return np.clip(action,-1,1)\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        #elite_states = elite_states.to(self.device)\n",
    "        elite_actions = torch.FloatTensor(np.array(elite_actions))\n",
    "        #elite_actions = elite_actions.to(self.device)\n",
    "        for i in range(15):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c375acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, exploration=0, visualize=False, custom_reward_func = None):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0, 'custom_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "    for i in range(trajectory_len):\n",
    "        with torch.no_grad():\n",
    "            action = agent.get_action(state, exploration)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "        if custom_reward_func:\n",
    "            reward = custom_reward_func(state) + reward\n",
    "            trajectory['custom_reward'] = np.max([trajectory['custom_reward'],reward])\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if visualize:\n",
    "            env.render()\n",
    "        \n",
    "        if(i != trajectory_len-1):\n",
    "            trajectory['states'].append(state)\n",
    "    \n",
    "    if visualize:\n",
    "        print(i,trajectory['total_reward'])  \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef5840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param, solved_traj):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param)\n",
    "    new_solved = [trajectory for trajectory in trajectories if trajectory['total_reward'] >= 200]\n",
    "    if(len(new_solved)):\n",
    "        solved_traj.extend(new_solved)\n",
    "        pass\n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] >= quantile]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ae427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous = True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "learnin_hist = {'mean_reward': [], 'exploration': [], 'validation': []}\n",
    "\n",
    "agent = CEM(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851e2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_len = 400\n",
    "q_param = 0.8\n",
    "exploration = 0.9\n",
    "e_rate = 0.993\n",
    "solved_traj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = -422.81537611477563\n",
      "solved = 0  elite len = 4\n",
      "validation = -356.7330049520963\n",
      "episode: 1, mean_total_reward = -335.77432698534824\n",
      "solved = 0  elite len = 4\n",
      "episode: 2, mean_total_reward = -321.9514957780235\n",
      "solved = 0  elite len = 4\n",
      "episode: 3, mean_total_reward = -329.96662559272033\n",
      "solved = 0  elite len = 4\n",
      "episode: 4, mean_total_reward = -138.1920758644001\n",
      "solved = 0  elite len = 4\n",
      "episode: 5, mean_total_reward = -81.84919516460168\n",
      "solved = 0  elite len = 4\n",
      "episode: 6, mean_total_reward = -79.66288025581527\n",
      "solved = 0  elite len = 4\n",
      "episode: 7, mean_total_reward = -48.977534519283104\n",
      "solved = 0  elite len = 4\n",
      "episode: 8, mean_total_reward = -107.79218290159699\n",
      "solved = 0  elite len = 4\n",
      "episode: 9, mean_total_reward = -64.08876000127812\n",
      "solved = 0  elite len = 4\n",
      "episode: 10, mean_total_reward = -1.565620451744665\n",
      "solved = 0  elite len = 4\n",
      "validation = -9.514935970949974\n",
      "episode: 11, mean_total_reward = -78.4386306715256\n",
      "solved = 0  elite len = 4\n",
      "episode: 12, mean_total_reward = -120.49779076474294\n",
      "solved = 0  elite len = 4\n",
      "episode: 13, mean_total_reward = -45.525478607685805\n",
      "solved = 0  elite len = 4\n",
      "episode: 14, mean_total_reward = -54.847870716222545\n",
      "solved = 0  elite len = 4\n",
      "episode: 15, mean_total_reward = -37.59814586740349\n",
      "solved = 0  elite len = 4\n",
      "episode: 16, mean_total_reward = -2.848896091279884\n",
      "solved = 0  elite len = 4\n",
      "episode: 17, mean_total_reward = -5.26696757213096\n",
      "solved = 0  elite len = 4\n",
      "episode: 18, mean_total_reward = 0.5191115140844019\n",
      "solved = 0  elite len = 4\n",
      "episode: 19, mean_total_reward = 22.290860532955605\n",
      "solved = 0  elite len = 4\n",
      "episode: 20, mean_total_reward = -9.64508856032958\n",
      "solved = 0  elite len = 4\n",
      "validation = -63.74794047323853\n",
      "episode: 21, mean_total_reward = 4.3783867405084065\n",
      "solved = 0  elite len = 4\n",
      "episode: 22, mean_total_reward = -0.9524256429203376\n",
      "solved = 0  elite len = 4\n",
      "episode: 23, mean_total_reward = -24.90446470050623\n",
      "solved = 0  elite len = 4\n",
      "episode: 24, mean_total_reward = -57.815892682646925\n",
      "solved = 0  elite len = 4\n",
      "episode: 25, mean_total_reward = -68.79339144369139\n",
      "solved = 0  elite len = 4\n",
      "episode: 26, mean_total_reward = -131.62378751247155\n",
      "solved = 0  elite len = 4\n",
      "episode: 27, mean_total_reward = 5.713583217628487\n",
      "solved = 0  elite len = 4\n",
      "episode: 28, mean_total_reward = -13.751545722785954\n",
      "solved = 0  elite len = 4\n",
      "episode: 29, mean_total_reward = 8.76513367417781\n",
      "solved = 0  elite len = 4\n",
      "episode: 30, mean_total_reward = -38.123401415000714\n",
      "solved = 0  elite len = 4\n",
      "validation = -39.206735929408616\n",
      "episode: 31, mean_total_reward = -10.596183932428385\n",
      "solved = 0  elite len = 4\n",
      "episode: 32, mean_total_reward = -97.19047243350369\n",
      "solved = 0  elite len = 4\n",
      "episode: 33, mean_total_reward = -129.9564948572068\n",
      "solved = 0  elite len = 4\n",
      "episode: 34, mean_total_reward = -85.08039005345816\n",
      "solved = 0  elite len = 4\n",
      "episode: 35, mean_total_reward = -39.104320177652035\n",
      "solved = 0  elite len = 4\n",
      "episode: 36, mean_total_reward = -158.91378252651705\n",
      "solved = 0  elite len = 4\n",
      "episode: 37, mean_total_reward = -154.34902661003062\n",
      "solved = 0  elite len = 4\n",
      "episode: 38, mean_total_reward = -44.31887439786932\n",
      "solved = 0  elite len = 4\n",
      "episode: 39, mean_total_reward = -18.738281679898552\n",
      "solved = 0  elite len = 4\n",
      "episode: 40, mean_total_reward = 48.52337247666744\n",
      "solved = 0  elite len = 4\n",
      "validation = -20.917594035255608\n",
      "episode: 41, mean_total_reward = 72.99054906899404\n",
      "solved = 0  elite len = 4\n",
      "episode: 42, mean_total_reward = 80.01386821678992\n",
      "solved = 0  elite len = 4\n",
      "episode: 43, mean_total_reward = 10.238360875054681\n",
      "solved = 0  elite len = 4\n",
      "episode: 44, mean_total_reward = 69.83204705436644\n",
      "solved = 0  elite len = 4\n",
      "episode: 45, mean_total_reward = 32.13078346837772\n",
      "solved = 0  elite len = 4\n",
      "episode: 46, mean_total_reward = 46.8873209570568\n",
      "solved = 0  elite len = 4\n",
      "episode: 47, mean_total_reward = 26.886787147155008\n",
      "solved = 0  elite len = 4\n",
      "episode: 48, mean_total_reward = 90.55781173911032\n",
      "solved = 0  elite len = 4\n",
      "episode: 49, mean_total_reward = 82.01592040811767\n",
      "solved = 0  elite len = 4\n",
      "episode: 50, mean_total_reward = 74.1837855218165\n",
      "solved = 0  elite len = 4\n",
      "validation = -8.773637504776719\n",
      "episode: 51, mean_total_reward = 68.57138612524085\n",
      "solved = 0  elite len = 4\n",
      "episode: 52, mean_total_reward = 79.63750862611352\n",
      "solved = 0  elite len = 4\n",
      "episode: 53, mean_total_reward = 66.12610250669574\n",
      "solved = 0  elite len = 4\n",
      "episode: 54, mean_total_reward = 97.73832114865981\n",
      "solved = 0  elite len = 4\n",
      "episode: 55, mean_total_reward = 93.4262869675263\n",
      "solved = 0  elite len = 4\n",
      "episode: 56, mean_total_reward = 72.75816744414401\n",
      "solved = 0  elite len = 4\n",
      "episode: 57, mean_total_reward = 100.51213963325601\n",
      "solved = 0  elite len = 4\n",
      "episode: 58, mean_total_reward = 109.09950027737698\n",
      "solved = 0  elite len = 4\n",
      "episode: 59, mean_total_reward = 85.91317753928499\n",
      "solved = 0  elite len = 4\n",
      "episode: 60, mean_total_reward = 46.27176328332278\n",
      "solved = 0  elite len = 4\n",
      "validation = 46.03340913502397\n",
      "episode: 61, mean_total_reward = 88.46004630030697\n",
      "solved = 0  elite len = 4\n",
      "episode: 62, mean_total_reward = 90.10868543867718\n",
      "solved = 0  elite len = 4\n",
      "episode: 63, mean_total_reward = 85.07044056005029\n",
      "solved = 0  elite len = 4\n",
      "episode: 64, mean_total_reward = 99.47075112406483\n",
      "solved = 0  elite len = 4\n",
      "episode: 65, mean_total_reward = 101.84493352278876\n",
      "solved = 0  elite len = 4\n",
      "episode: 66, mean_total_reward = 81.21714334678222\n",
      "solved = 0  elite len = 4\n",
      "episode: 67, mean_total_reward = 123.7327988846263\n",
      "solved = 0  elite len = 4\n",
      "episode: 68, mean_total_reward = 58.35007308115503\n",
      "solved = 0  elite len = 4\n",
      "episode: 69, mean_total_reward = 53.325814695269756\n",
      "solved = 0  elite len = 4\n",
      "episode: 70, mean_total_reward = 11.055629328711074\n",
      "solved = 0  elite len = 4\n",
      "validation = -23.848873132208787\n",
      "episode: 71, mean_total_reward = 46.16104801533511\n",
      "solved = 0  elite len = 4\n",
      "episode: 72, mean_total_reward = 17.745024333896275\n",
      "solved = 0  elite len = 4\n",
      "episode: 73, mean_total_reward = 58.429593648430135\n",
      "solved = 0  elite len = 4\n",
      "episode: 74, mean_total_reward = 71.98207478792116\n",
      "solved = 0  elite len = 4\n",
      "episode: 75, mean_total_reward = 85.24042947550579\n",
      "solved = 1  elite len = 4\n",
      "episode: 76, mean_total_reward = 85.45115922306368\n",
      "solved = 1  elite len = 4\n",
      "episode: 77, mean_total_reward = 114.00126342124204\n",
      "solved = 1  elite len = 4\n",
      "episode: 78, mean_total_reward = 133.6803890704448\n",
      "solved = 1  elite len = 4\n",
      "episode: 79, mean_total_reward = 122.91991423840709\n",
      "solved = 1  elite len = 4\n",
      "episode: 80, mean_total_reward = 139.847881653976\n",
      "solved = 2  elite len = 4\n",
      "validation = 73.49984525396084\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episode_n = 1000\n",
    "trajectory_n = 20\n",
    "\n",
    "for episode in range(0,episode_n):\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len, exploration) for _ in range(trajectory_n)]\n",
    "    exploration *= e_rate\n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param, solved_traj)\n",
    "    print('solved =',len(solved_traj),' elite len =',len(elite_trajectories))\n",
    "    learnin_hist['exploration'].append([episode,mean_total_reward])\n",
    "    learnin_hist['mean_reward'].append([episode,mean_total_reward])\n",
    "    if episode%10==0:\n",
    "        trajectories = [get_trajectory(env, agent, trajectory_len, -1) for _ in range(trajectory_n*3)]\n",
    "        mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "        if(mean_total_reward>2600):\n",
    "            break\n",
    "        learnin_hist['validation'].append([episode,mean_total_reward])\n",
    "        print('validation =', mean_total_reward)\n",
    "    if len(elite_trajectories) > 0:\n",
    "        if(len(solved_traj)>0):\n",
    "            elite_trajectories.append(solved_traj[np.random.randint(len(solved_traj))])\n",
    "            agent.update_policy(elite_trajectories)\n",
    "        else:\n",
    "            agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29665a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.array(learnin_hist['mean_reward'])\n",
    "hist = hist[:400,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceefc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[:,0] *= trajectory_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ce4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist[:,0],hist[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaed547",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('ce3.txt',hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee77f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c233e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42392ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dcdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22052ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
